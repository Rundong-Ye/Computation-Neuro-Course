这里对20180905课上同学们提的一些问题进行一些解释。我尽量把我知道的讲给大家，但是我的知识很有限，大家感兴趣的话可以进一步在网上查找相关资料。

------------

首先，我们这门课时讲 计算神经科学(computational neuroscience) 的，与人工智能中的 人工神经网络(artificial neural network ANN) 是不同的。
ANN是受到神经科学的启发发明（这是一类算法或者技术，所以称为“发明”，而非“发现”）出来的，但是并非是完全对生物神经网络的模拟。
人工智能是一门工程科学，它的目的是发明一些好（这个“好”的标准是不断发展的，也经常从人脑的功能借鉴而来，最基本的包括快和准）的算法，
而不在乎算法是否和生物神经网络“像”（也有少数做类脑计算的老师比较在乎这个）。事实上，历史上，经常是人工智能从神经科学借鉴一个想法，
然后为了提高算法的效果，不断改进算法，与生物神经网络渐行渐远，过一段时间后，又从神经科学借鉴一个新想法。
比如开始的AlexNet，还是有点像人的视觉系统（V1之前），但是后来为了尺度不变性GoogLeNet进行了一些优化，为了反向传播的有效性ResNet进行了一些优化，
就越来越不像人的视觉系统了，加入了很多生物上无法实现的机制。最近Hinton提出capsule，又加入了很多生物神经网络的特点。
而神经科学，包括计算神经科学，目的是研究生物神经网络，搞清楚它的原理，它完成了什么功能，用什么算法完成的，是如何通过一团物质实现这些算法的，等等。
虽然有很多理论神经科学的工作利用第一性原理去提出一些算法，但是这些算法最终是要证实它是生物所采取的算法，或者对我们理解生物神经网络又帮助，才有意义。
仅仅是提出一个高效的算法，就不属于神经科学了。大家在看人工智能的一些算法和论文时要注意，无论它怎么说它是brain inspired，也不意味着大脑就是这样工作的。

第一个问题，卷积层可视化：
参考 https://blog.csdn.net/MargretWG/article/details/69586356

很多地方我们都会看到类似温老师slides里给的图片，CNN训练出来的类似一个一个棒棒的feature。这些都是对CNN的第一层的可视化，就是最简单的，
把第一层的weights（对于CNN来说就是卷积核）画出来了。这个方法只适用于第一层，因为后面的层有了非线性激活函数，仅画weights就没有意义了。
更复杂的反卷积方法可以参考上面的链接。对于第一层，weights就代表了它所偏好的模式，因为卷积可以理解为一种模板匹配的操作。

第二个问题，非线性激活函数的意义：

有同学问为什么要在线性求和外面加一个非线性激活函数。对于ANN来说，我们可以这样理解，如果每一层都只是线性变换，那么无论多少层叠加在一起，
都可以等价于一层线性变换。这使得这个网络的运算非常简单，无法拟合数据的真实分布。
对于生物神经网络，实际上并不是这样简单。在很多假设条件下的简化，可以得到这种结果（线性叠加，外面套一个非线性激活函数）。
可以参考温老师给的参考教材
Theoretical Neuroscience: Computational and Mathematical Modeling the Neural System 7.2节。
对于生物神经网络，其实也很难去问为什么生物采取这样的算法，我们可能更关心它到底采用了什么算法。当然有时候，根据进化论的思想，
去问“为什么生物采取这样的算法”，可以给我们带来很多启示。
